#pip install PyGithub
from github import Github
import logging
import os
#pip install streamlit
import streamlit as st
from openai import OpenAI
import json

#streamlit run .\chat_with_github.py
#run this script with streamlit, it will start a web server and you can access it via http://localhost:8501

def get_repo_tree(repo_full_name, branch=None):
    # ä»ç¯å¢ƒå˜é‡é‡Œè¯»å–githubçš„token
    #token = os.getenv("GITHUB_TOKEN", None)
    # åˆ›å»ºä¸€ä¸ªgithubå¯¹è±¡
    #g = Github(token)
    g = Github()  # ä½¿ç”¨æ— è®¤è¯çš„æ–¹å¼è·å–å…¬å…±ä»“åº“ä¿¡æ¯
    # è·å–ä¸€ä¸ªrepoå¯¹è±¡
    repo = g.get_repo(f"{repo_full_name}")
    # è·å–repoçš„ç›®å½•ç»“æ„
    # Ensure branch is not None
    if branch is None:
        branch = "master"
    tree = repo.get_git_tree(sha=branch, recursive=True)
    # æ„é€ æ ‘å½¢å¼çš„å­—ç¬¦ä¸²
    tree_str = ""
    for item in tree.tree:
        tree_str += f"{item.path}\n"
    return tree_str

#tree = get_repo_tree("ollama/ollama", "main")
#print(tree)

def get_repo_file_content(repo_full_name, file_path, branch=None):
    # ä»ç¯å¢ƒå˜é‡é‡Œè¯»å–githubçš„token
    #token = os.getenv("GITHUB_TOKEN", None)
    # åˆ›å»ºä¸€ä¸ªgithubå¯¹è±¡
    #g = Github(token)
    g = Github()  # ä½¿ç”¨æ— è®¤è¯çš„æ–¹å¼è·å–å…¬å…±ä»“åº“ä¿¡æ¯
    # è·å–ä¸€ä¸ªrepoå¯¹è±¡
    repo = g.get_repo(f"{repo_full_name}")
    # è·å–æ–‡ä»¶å†…å®¹
    # Ensure branch is not None
    if branch is None:
        branch = "master"
    file_content = repo.get_contents(file_path, ref=branch)
    return file_content.decoded_content.decode("utf-8")

#file_content = get_repo_file_content("ollama/ollama", "README.md", "main")
#print(file_content)

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_repo_tree",
            "description": "è·å–æŒ‡å®šGitHubä»“åº“çš„ç›®å½•ç»“æ„",
            "parameters": {
                "type": "object",
                "properties": {
                    "repo_full_name": {
                        "type": "string",
                        "description": "GitHubä»“åº“çš„å…¨åï¼Œä¾‹å¦‚ 'owner/repo'",
                    },
                    "branch": {
                        "type": "string",
                        "description": "æŒ‡å®šåˆ†æ”¯åç§°ï¼Œé»˜è®¤ä¸º 'master'",
                    },
                },
                "required": ["repo_full_name"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_repo_file_content",
            "description": "è·å–æŒ‡å®šGitHubä»“åº“ä¸­æŸä¸ªæ–‡ä»¶çš„å†…å®¹",
            "parameters": {
                "type": "object",
                "properties": {
                    "repo_full_name": {
                        "type": "string",
                        "description": "GitHubä»“åº“çš„å…¨åï¼Œä¾‹å¦‚ 'owner/repo'",
                    },
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡ä»¶åœ¨ä»“åº“ä¸­çš„è·¯å¾„ï¼Œä¾‹å¦‚ 'path/to/file.txt'",
                    },
                    "branch": {
                        "type": "string",
                        "description": "æŒ‡å®šåˆ†æ”¯åç§°ï¼Œé»˜è®¤ä¸º 'master'",
                    },
                },
                "required": ["repo_full_name", "file_path"],
            },
        },
    },
]

available_functions = {
    "get_repo_tree": get_repo_tree,
    "get_repo_file_content": get_repo_file_content,
}

client = OpenAI(
    api_key="123",
    base_url="http://localhost:11434/v1",  # local Ollama server
)

MODEL_NAME = "qwen3:4b"

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

logging.info("Starting Streamlit script ...")

st.set_page_config(
    page_title="Chat with GitHub",
    page_icon="ğŸ¤–",
    layout="wide",
)

st.subheader("Chat with GitHub - just a demo")

if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©ç”¨æˆ·è§£è¯»Githubä¸Šä»£ç åº“çš„è½¯ä»¶ä¸“å®¶ã€‚"}]
else:
    for message in st.session_state.messages:
        if message["role"] == "user" or message["role"] == "assistant":
            if "content" in message.keys():
                with st.chat_message(message["role"]):
                    st.write(message["content"])

if user_prompt := st.chat_input('åœ¨æ­¤è¾“å…¥ä½ çš„é—®é¢˜'):
    logging.info(f"User prompt: {user_prompt}")
    with st.chat_message("user"):
        st.write(user_prompt)
        st.session_state.messages.append({"role": "user", "content": user_prompt})
    
    with st.chat_message("assistant"):
        with st.spinner("äººå·¥æ™ºèƒ½åœ¨æ€è€ƒ...."):
            gpt_response = client.chat.completions.create(
                model = MODEL_NAME,
                messages = st.session_state.messages,
                tools = tools
            )

            response_msg = gpt_response.choices[0].message
            tool_calls = response_msg.tool_calls

            if tool_calls:
                st.session_state.messages.append({"role": "assistant", "tool_calls": response_msg.tool_calls})
                for tool_call in tool_calls:
                    function_args = json.loads(tool_call.function.arguments)
                    logging.info(f"Function call: {tool_call.function.name} with arguments: {function_args}")
                    if tool_call.function.name == "get_repo_tree":
                        function_response = available_functions["get_repo_tree"](function_args.get("repo_full_name"), function_args.get("branch"))
                    if tool_call.function.name == "get_repo_file_content":
                        function_response = available_functions["get_repo_file_content"](function_args.get("repo_full_name"), function_args.get("file_path"), function_args.get("branch"))
                    logging.info(f"Function {tool_call.function.name} responsed")
                    st.session_state.messages.append(
                        {
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": tool_call.function.name,
                            "content": function_response,
                        })
                secondary_gpt_response = client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=st.session_state.messages,
                    )
                secondary_gpt_response_msg = secondary_gpt_response.choices[0].message
                if secondary_gpt_response_msg.role == "assistant":
                    st.write(secondary_gpt_response_msg.content)
                    st.session_state.messages.append({"role": "assistant", "content": secondary_gpt_response_msg.content})
            else:
                if response_msg.role == "assistant":
                    st.write(response_msg.content)
                    st.session_state.messages.append({"role": "assistant", "content": response_msg.content})

st.write("**æ³¨æ„ï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ä»…ä¾›å‚è€ƒã€‚**")
logging.info("Streamlit script ended.")